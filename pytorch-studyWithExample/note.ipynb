{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('torchEnv')",
   "display_name": "Python 3.8.5 64-bit ('torchEnv')",
   "metadata": {
    "interpreter": {
     "hash": "2c92d966cd6d614aca0ee7d626700fdc5945b0d74bba3d9cf69f3cd79513d971"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# numpy 사용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 33172623.66426563\n1 35121999.28960111\n2 42299695.820782706\n3 46474377.599419236\n4 39164549.208869286\n5 23495372.248044875\n6 10405012.095174978\n7 4325365.418492799\n8 2140995.83325008\n9 1362278.3865940957\n10 1018862.1698861377\n11 820943.6589290784\n12 682412.2627981608\n13 575525.084357893\n14 489728.8914945789\n15 419480.55425753084\n16 361233.6374714101\n17 312541.1296923335\n18 271574.04019493714\n19 236963.3137993855\n20 207512.5876530419\n21 182378.04118084238\n22 160814.25275062938\n23 142234.1125555866\n24 126154.65639385625\n25 112173.9938708458\n26 99994.92136965325\n27 89346.03080265477\n28 80009.47235520533\n29 71795.23877049563\n30 64552.81006901731\n31 58154.02384320396\n32 52487.40357620381\n33 47453.53729460374\n34 42975.462749358485\n35 38978.53958534695\n36 35404.28942380005\n37 32204.536070441438\n38 29335.34082865022\n39 26754.48170465466\n40 24429.194704676316\n41 22331.469398212088\n42 20437.60116522782\n43 18722.95565925293\n44 17170.199964606425\n45 15761.7502677457\n46 14482.583345488776\n47 13319.455133860256\n48 12261.215746846416\n49 11296.196200982988\n50 10415.942445153505\n51 9611.73412609337\n52 8875.985237008983\n53 8201.890726288899\n54 7584.665602915042\n55 7019.296080850601\n56 6500.387872893451\n57 6023.8672079769585\n58 5586.198452092911\n59 5183.241831095831\n60 4812.158232125127\n61 4470.262647590009\n62 4154.931487985882\n63 3864.1180525020573\n64 3595.616800804685\n65 3347.4230614231083\n66 3117.92850523874\n67 2905.612209972387\n68 2709.6160409248478\n69 2527.9535514440936\n70 2359.6432374527244\n71 2203.5427663318865\n72 2058.8697032120012\n73 1924.4550484722004\n74 1799.5704867100462\n75 1683.4717906587166\n76 1575.516200837365\n77 1475.0394858206103\n78 1381.5127358396874\n79 1294.518285113791\n80 1213.3546616491103\n81 1137.7044461968765\n82 1067.1531064980252\n83 1001.3316958483247\n84 939.9411158794268\n85 882.5759200597137\n86 829.0011676392446\n87 778.9398789426771\n88 732.1595242186545\n89 688.4181072561321\n90 647.4967854116273\n91 609.1476656634658\n92 573.2477114920509\n93 539.6158138625515\n94 508.11186394934725\n95 478.60426012773667\n96 450.92442155307924\n97 424.9526784171843\n98 400.5917689705883\n99 377.734468353459\n100 356.3090764751278\n101 336.157397727345\n102 317.22336714742937\n103 299.4377491532878\n104 282.7234873022958\n105 267.0014093978368\n106 252.22133946169967\n107 238.31725798862152\n108 225.2357499832964\n109 212.91687698354554\n110 201.34311449280128\n111 190.42599634175642\n112 180.13870669246046\n113 170.44405030501215\n114 161.30830985751217\n115 152.69676161158807\n116 144.57515046152645\n117 136.9146335928461\n118 129.68923499409505\n119 122.8725710121652\n120 116.45223545217846\n121 110.37640743394121\n122 104.63404902618782\n123 99.20823609836302\n124 94.08145822802658\n125 89.23727773738773\n126 84.65913943186307\n127 80.33043459901779\n128 76.23731622779364\n129 72.36940997388609\n130 68.71563695433177\n131 65.24950303013475\n132 61.96943194184617\n133 58.86550618809957\n134 55.92692876607995\n135 53.14324535301972\n136 50.50615406943044\n137 48.008161142023866\n138 45.64185009438914\n139 43.401062950026414\n140 41.27929288343659\n141 39.26239811176967\n142 37.350363047330156\n143 35.53673706929041\n144 33.816103205316736\n145 32.18334211788712\n146 30.634627938857978\n147 29.164335952642816\n148 27.768883113595614\n149 26.444679598855984\n150 25.188572982135\n151 23.992871333224457\n152 22.857235119123132\n153 21.778386005036502\n154 20.75279583952831\n155 19.778177029414252\n156 18.851612750032565\n157 17.970919623514146\n158 17.133418663131554\n159 16.338157252003143\n160 15.58198787981205\n161 14.861122692244802\n162 14.175286780294853\n163 13.522931589588143\n164 12.90169707025645\n165 12.310442491489134\n166 11.747597662324463\n167 11.211888897210777\n168 10.701502917123069\n169 10.21613030748162\n170 9.754148730502177\n171 9.313178173388543\n172 8.892924108741175\n173 8.492490938200017\n174 8.110999948724642\n175 7.74729842848365\n176 7.400596695588467\n177 7.0701794732560685\n178 6.755018690983883\n179 6.454826233493521\n180 6.168893099466889\n181 5.895553229837802\n182 5.634769439873011\n183 5.386026349363265\n184 5.148733431111594\n185 4.9222572501084665\n186 4.706217235033771\n187 4.499926161689984\n188 4.303005547860783\n189 4.115087220986258\n190 3.9362779420014355\n191 3.764911403011576\n192 3.601267177475367\n193 3.444966011146751\n194 3.295693949328995\n195 3.1531513208544717\n196 3.01699539968903\n197 2.886937053100393\n198 2.762630678613423\n199 2.6438472991308695\n200 2.530617340453135\n201 2.4222749234648138\n202 2.3185837615555362\n203 2.219466927990562\n204 2.1247217167283505\n205 2.0341962872840176\n206 1.9476227460291107\n207 1.8648296727714238\n208 1.7856619320080778\n209 1.7099405853887666\n210 1.6375415578310053\n211 1.5685348706901174\n212 1.5022905600957106\n213 1.4389183861007497\n214 1.3782973559421867\n215 1.3203050315158138\n216 1.2647953666978289\n217 1.2116844692915856\n218 1.160880482768468\n219 1.1122480407369304\n220 1.0656988735662172\n221 1.021210252278744\n222 0.978690980276093\n223 0.9378678404345504\n224 0.8988032404393235\n225 0.8613979752670218\n226 0.825574114451218\n227 0.7912797946384059\n228 0.7584440690167755\n229 0.7270057178303584\n230 0.6968977056513947\n231 0.6680682253499548\n232 0.6404933917881793\n233 0.6141070954894214\n234 0.5887720881891629\n235 0.5644999403000588\n236 0.5412457848615535\n237 0.5189739735688985\n238 0.4976330706174444\n239 0.477188421852689\n240 0.4576130113869157\n241 0.438847959282864\n242 0.4208678400580862\n243 0.40366436759420027\n244 0.3871893411266486\n245 0.37135735706862005\n246 0.3561868613041852\n247 0.34164666071701194\n248 0.32770987924465556\n249 0.3143533323289529\n250 0.3015518208572955\n251 0.2892812128729665\n252 0.2775183273633045\n253 0.26623808104037205\n254 0.25542511173927007\n255 0.24509759646106377\n256 0.23515516047139354\n257 0.22562186293472852\n258 0.21648312447435458\n259 0.2077194599723516\n260 0.19931989948505338\n261 0.19126332102585497\n262 0.18353366721756628\n263 0.1761232157331183\n264 0.16901514477371488\n265 0.1621972446441638\n266 0.15567401711250015\n267 0.14941340187461327\n268 0.1433969234305873\n269 0.13762661521804012\n270 0.13209145501567005\n271 0.1267813410155671\n272 0.12168744966312092\n273 0.11680053823200984\n274 0.11211303663066703\n275 0.10761510847286751\n276 0.10330003989298861\n277 0.09916158289464132\n278 0.09520279765120027\n279 0.09139269475783879\n280 0.08773497056874431\n281 0.08422586045105868\n282 0.08085841269336184\n283 0.07762660054784043\n284 0.07452545624573559\n285 0.07155052457189577\n286 0.06869530533832385\n287 0.06595462390342585\n288 0.06332424124436649\n289 0.060801377347249344\n290 0.058386705693096036\n291 0.056060947008201856\n292 0.053829944413476584\n293 0.05168849676744138\n294 0.04963227771540157\n295 0.04765831967332485\n296 0.045764019789037935\n297 0.04394535107391684\n298 0.042199660259419194\n299 0.04052399330127468\n300 0.03891519497996612\n301 0.037371154158003766\n302 0.03589482394309105\n303 0.03447138428900431\n304 0.03310494326369379\n305 0.031793237992305656\n306 0.03053361090054314\n307 0.029324699181843182\n308 0.028163841094614138\n309 0.027049113752702983\n310 0.025979265399140215\n311 0.02495163388566971\n312 0.02396507746609172\n313 0.023017807965666437\n314 0.02211032334724502\n315 0.02123828263164492\n316 0.02039940696555447\n317 0.019593897788833987\n318 0.018820874075740603\n319 0.018078312351168892\n320 0.017365030029875956\n321 0.016680175654102423\n322 0.016022634154837984\n323 0.015390969734301171\n324 0.01478440081919044\n325 0.01420189914310727\n326 0.01364314922227882\n327 0.013107366702592941\n328 0.012591278025266484\n329 0.012095659824850417\n330 0.011619705007861542\n331 0.011162618353848925\n332 0.010723535155683236\n333 0.01030186270291386\n334 0.009896855407481654\n335 0.009507899432789565\n336 0.009134244417662417\n337 0.008775328558404473\n338 0.008430598725356512\n339 0.008100216139926234\n340 0.007782623945269822\n341 0.007477152345338321\n342 0.007183748775602162\n343 0.00690181689132939\n344 0.006631068438909041\n345 0.006370994174301782\n346 0.006121105856373449\n347 0.005881103384766004\n348 0.005650542883113573\n349 0.005429080183946373\n350 0.005216323552191041\n351 0.0050119177096227645\n352 0.004816203890721815\n353 0.004627645024819264\n354 0.004446406776860842\n355 0.004272295229658651\n356 0.00410505083791378\n357 0.003944355540791944\n358 0.0037899884620899178\n359 0.0036416729253352076\n360 0.0034992232297537448\n361 0.0033623394785671436\n362 0.0032308267679174727\n363 0.003104470746316914\n364 0.0029831043155746253\n365 0.0028668988606272636\n366 0.002754841751296895\n367 0.002647164564160617\n368 0.0025437074276793937\n369 0.002444327487170254\n370 0.002348834789340595\n371 0.0022570970076154183\n372 0.0021689756884146843\n373 0.0020842645550120933\n374 0.002002871881552117\n375 0.001924684117826321\n376 0.0018495547526227478\n377 0.0017773666702891495\n378 0.0017082472706078438\n379 0.001641614358678849\n380 0.0015775575458362638\n381 0.0015160124193511872\n382 0.0014568713241779698\n383 0.001400060668330626\n384 0.0013454579402174987\n385 0.001292989641662838\n386 0.0012425843379263073\n387 0.0011941389375723817\n388 0.0011475882424563104\n389 0.0011028666392586535\n390 0.001059883457649876\n391 0.0010187035616048223\n392 0.000979039493960049\n393 0.0009409021294871007\n394 0.0009042510325207171\n395 0.0008690282178018336\n396 0.0008351759894631074\n397 0.0008026555109333987\n398 0.0007713992379788604\n399 0.0007413680813427508\n400 0.0007125091998282261\n401 0.0006847740871213904\n402 0.0006581161053143199\n403 0.0006325031731898584\n404 0.0006079235336487904\n405 0.0005843191597721302\n406 0.0005615813291523313\n407 0.0005397343262764552\n408 0.000518735926647023\n409 0.0004985563077570054\n410 0.0004791619812308214\n411 0.00046052718420203564\n412 0.00044261572732718393\n413 0.00042540338582963883\n414 0.00040886575707315483\n415 0.0003929688930435892\n416 0.0003776902730867179\n417 0.0003630082435936507\n418 0.00034893173399672215\n419 0.00033538398673097993\n420 0.0003223482974865484\n421 0.00030982055972211093\n422 0.0002977809328468521\n423 0.000286210609390668\n424 0.0002750887644295775\n425 0.0002644020642633762\n426 0.00025413114434735286\n427 0.0002442608587506695\n428 0.00023477326113759768\n429 0.0002256549761727642\n430 0.00021689029657352484\n431 0.00020846952676460391\n432 0.00020040027716441457\n433 0.0001926193138718655\n434 0.00018514074163038658\n435 0.00017795258269027696\n436 0.0001710440362798646\n437 0.00016440366074386132\n438 0.00015802072369475103\n439 0.0001518877959341906\n440 0.0001459916798548052\n441 0.00014032545118651592\n442 0.00013488104772062202\n443 0.0001296468376168958\n444 0.00012461534341581907\n445 0.00011978028942537556\n446 0.00011514869634681974\n447 0.0001106810254106532\n448 0.00010638681300113554\n449 0.00010226019760919868\n450 9.829365494972845e-05\n451 9.448117689952651e-05\n452 9.081637989714333e-05\n453 8.729421634919458e-05\n454 8.390817909724894e-05\n455 8.065476331645116e-05\n456 7.752755524171142e-05\n457 7.452104758976573e-05\n458 7.16313799795097e-05\n459 6.885414558771216e-05\n460 6.619174897831678e-05\n461 6.362745090960656e-05\n462 6.116062116180525e-05\n463 5.8789633612546994e-05\n464 5.651094579783129e-05\n465 5.4320304503528575e-05\n466 5.221467712232879e-05\n467 5.019108955102573e-05\n468 4.8245663339196854e-05\n469 4.637583537052046e-05\n470 4.457885365615673e-05\n471 4.285112692797105e-05\n472 4.1190750428325874e-05\n473 3.959457980344326e-05\n474 3.806397213492073e-05\n475 3.659114015080084e-05\n476 3.517325034423146e-05\n477 3.381043014240893e-05\n478 3.250074428900986e-05\n479 3.1241588932852445e-05\n480 3.0031404585733518e-05\n481 2.8868069266459005e-05\n482 2.7749788362138112e-05\n483 2.6675309032632947e-05\n484 2.5642198794471684e-05\n485 2.464889883349342e-05\n486 2.369428352433218e-05\n487 2.2776721891114326e-05\n488 2.1896060925287193e-05\n489 2.1049701103630716e-05\n490 2.0234673072894707e-05\n491 1.9451064719767096e-05\n492 1.86979130024577e-05\n493 1.797389983574595e-05\n494 1.7277954313486467e-05\n495 1.6609093590827924e-05\n496 1.596607335087245e-05\n497 1.5348019802326028e-05\n498 1.4753856279644303e-05\n499 1.4182663265794053e-05\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "batchSize, inputLayer, hiddenLayer, outputLayer = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(batchSize, inputLayer)\n",
    "y = np.random.randn(batchSize, outputLayer)\n",
    "\n",
    "w1 = np.random.randn(inputLayer, hiddenLayer)\n",
    "w2 = np.random.randn(hiddenLayer, outputLayer)\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "for epoch in range(500):\n",
    "    # 순전파 단계: 예측값 y 계산\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    #  Loss를 계산하고 출력\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "source": [
    "# pytorch 사용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "print(device)\n",
    "\n",
    "batchSize, inputLayer, hiddenLayer, outputLayer = 64, 1000, 100, 10\n",
    "\n",
    "lr = 1e-6\n",
    "x = torch.randn(batchSize, inputLayer, device=device, dtype=dtype)\n",
    "y = torch.randn(batchSize, outputLayer, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(inputLayer, hiddenLayer, device=device, dtype=dtype)\n",
    "w2 = torch.randn(hiddenLayer, outputLayer, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "for epoch in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if epoch % 100 == 99: print(epoch, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다. \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법을 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n99 461.3334655761719\n199 1.8259186744689941\n299 0.010903755202889442\n399 0.0002197134162997827\n499 3.5271506931167096e-05\n"
    }
   ]
  },
  {
   "source": [
    "# Autograd\n",
    "\n",
    "Autograd를 사용하여 신경망에서 역전파 단계 연산을 자동화 할 수 있습니다. Autograd를 사용할 때, 신경망의 순전파 단계는 연산 그래프를 정의하게 됩니다. 이 그래프의 노드는 Tensor, 엣지는 입력 Tensor로부터 출력 Tensor를 만들어내는 함수가 됩니다. 이 그래프를 통해 역전파를 하게되면 변화도를 쉽게 계산할 수 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99 647.7728271484375\n199 3.8132331371307373\n299 0.029090795665979385\n399 0.0004981890087947249\n499 6.34252282907255e-05\n"
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batchSize, inputLayer, hiddenLayer, outputLayer = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(batchSize, inputLayer, device=device, dtype=dtype)\n",
    "y = torch.randn(batchSize, outputLayer, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(inputLayer, hiddenLayer, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(hiddenLayer, outputLayer, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if epoch % 100 == 99: print(epoch, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "source": [
    "# 새로운 Autograd 함수 정의\n",
    "\n",
    "내부적으로, autograd의 기본 연산자는 실제로 Tensor를 조작하는 2개의 함수입니다. forward 함수는 입력 Tensor로부터 출력 Tensor를 계산합니다. backward 함수는 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 전달받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산합니다.\n",
    "\n",
    "PyTorch에서 torch.autograd.Function의 서브클래스를 정의하고 forward와 backward를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게 정의할 수 있습니다. 그 후, 인스턴스를 생성하고 이를 함수처럼 호출하여 입력 데이터를 갖는 Tensor를 전달하는 식으로 새로운 autograd 연산자를 사용할 수 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_):\n",
    "        ctx.save_for_backward(input_)\n",
    "        return input_.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_ < 0] = 0\n",
    "\n",
    "        return grad_input"
   ]
  }
 ]
}